{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Welcome to the S2AND tutorial!\n",
    "\n",
    "We will cover a few aspects of the S2AND pipeline:\n",
    "\n",
    "(1) Load a test dataset.\n",
    "(2) Fit a pairwise model + bells & whistles.\n",
    "(3) Fit a clusterer.\n",
    "(4) Evaluate the pairwise model and the clusterer.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "from typing import Dict, Any, Optional, List\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from s2and.data import ANDData\n",
    "from s2and.featurizer import featurize, FeaturizationInfo\n",
    "from s2and.model import PairwiseModeler, Clusterer, FastCluster\n",
    "from s2and.eval import pairwise_eval, cluster_eval, facet_eval\n",
    "from s2and.consts import FEATURIZER_VERSION, DEFAULT_CHUNK_SIZE, PROJECT_ROOT_PATH\n",
    "from s2and.file_cache import cached_path\n",
    "from s2and.plotting_utils import plot_facets\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the random seed we used for the ablations table\n",
    "random_seed = 42\n",
    "# number of cpus to use\n",
    "n_jobs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 12:32:59,849 - s2and - INFO - loading papers\n",
      "2020-12-16 12:33:01,273 - s2and - INFO - loaded papers\n",
      "2020-12-16 12:33:01,274 - s2and - INFO - loading signatures\n",
      "2020-12-16 12:33:01,426 - s2and - INFO - loaded signatures\n",
      "2020-12-16 12:33:01,429 - s2and - INFO - loading clusters\n",
      "2020-12-16 12:33:01,437 - s2and - INFO - loaded clusters, loading specter\n",
      "2020-12-16 12:33:01,509 - s2and - INFO - loaded specter, loading cluster seeds\n",
      "2020-12-16 12:33:01,510 - s2and - INFO - loaded cluster seeds\n",
      "2020-12-16 12:33:01,511 - s2and - INFO - making signature to cluster id\n",
      "2020-12-16 12:33:01,513 - s2and - INFO - made signature to cluster id\n",
      "2020-12-16 12:33:01,514 - s2and - INFO - loading name counts\n",
      "2020-12-16 12:33:24,297 - s2and - INFO - loaded name counts\n",
      "2020-12-16 12:33:24,530 - s2and - INFO - preprocessing papers\n",
      "Preprocessing papers 1/2: 100%|██████████| 53959/53959 [00:03<00:00, 13828.07it/s]\n",
      "Preprocessing papers 2/2: 100%|██████████| 53959/53959 [00:09<00:00, 5990.95it/s] \n",
      "2020-12-16 12:33:43,932 - s2and - INFO - preprocessed papers\n",
      "2020-12-16 12:33:43,934 - s2and - INFO - preprocessing signatures\n",
      "Preprocessing signatures: 100%|██████████| 7144/7144 [00:01<00:00, 5012.76it/s]\n",
      "2020-12-16 12:33:45,364 - s2and - INFO - preprocessed signatures\n"
     ]
    }
   ],
   "source": [
    "# we're going to load the arnetminer dataset\n",
    "# and assume that you have it already downloaded to the `S2AND/data/` directory\n",
    "\n",
    "dataset_name = 'arnetminer'\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT_PATH, 'data', dataset_name)\n",
    "\n",
    "anddata = ANDData(\n",
    "    signatures=os.path.join(DATA_DIR, dataset_name + \"_signatures.json\"),\n",
    "    papers=os.path.join(DATA_DIR, dataset_name + \"_papers.json\"),\n",
    "    name=dataset_name,\n",
    "    mode=\"train\",  # can also be 'inference' if just predicting\n",
    "    specter_embeddings=os.path.join(DATA_DIR, dataset_name + \"_specter.pickle\"),\n",
    "    clusters=os.path.join(DATA_DIR, dataset_name + \"_clusters.json\"),\n",
    "    block_type=\"s2\",  # can also be 'original'\n",
    "    train_pairs=None,  # in case you have predefined splits for the pairwise models\n",
    "    val_pairs=None,\n",
    "    test_pairs=None,\n",
    "    train_pairs_size=100000,  # how many training pairs for the pairwise models?\n",
    "    val_pairs_size=10000,\n",
    "    test_pairs_size=10000,\n",
    "    n_jobs=n_jobs,\n",
    "    load_name_counts=True,  # the name counts derived from the entire S2 corpus need to be loaded separately\n",
    "    preprocess=True,\n",
    "    random_seed=random_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 12:33:46,027 - s2and - INFO - featurizing train\n",
      "2020-12-16 12:33:46,040 - s2and - INFO - Creating 100000 pieces of work\n",
      "2020-12-16 12:33:46,137 - s2and - INFO - Created pieces of work\n",
      "2020-12-16 12:33:46,138 - s2and - INFO - Cached changed, doing 100000 work in parallel\n",
      "Doing work: 100%|██████████| 100000/100000 [00:14<00:00, 6732.01it/s]\n",
      "2020-12-16 12:34:01,778 - s2and - INFO - Work completed\n",
      "2020-12-16 12:34:01,780 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2020-12-16 12:34:01,863 - s2and - INFO - Numpy arrays made\n",
      "2020-12-16 12:34:01,882 - s2and - INFO - featurized train, featurizing val\n",
      "2020-12-16 12:34:01,885 - s2and - INFO - Creating 10000 pieces of work\n",
      "2020-12-16 12:34:01,917 - s2and - INFO - Created pieces of work\n",
      "2020-12-16 12:34:01,918 - s2and - INFO - Cached changed, doing 10000 work in parallel\n",
      "2020-12-16 12:34:04,273 - s2and - INFO - Work completed\n",
      "2020-12-16 12:34:04,275 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2020-12-16 12:34:04,294 - s2and - INFO - Numpy arrays made\n",
      "2020-12-16 12:34:04,297 - s2and - INFO - featurized val, featurizing test\n",
      "2020-12-16 12:34:04,298 - s2and - INFO - Creating 10000 pieces of work\n",
      "2020-12-16 12:34:04,318 - s2and - INFO - Created pieces of work\n",
      "2020-12-16 12:34:04,319 - s2and - INFO - Cached changed, doing 10000 work in parallel\n",
      "2020-12-16 12:34:06,454 - s2and - INFO - Work completed\n",
      "2020-12-16 12:34:06,455 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2020-12-16 12:34:06,470 - s2and - INFO - Numpy arrays made\n",
      "2020-12-16 12:34:06,472 - s2and - INFO - featurized test\n"
     ]
    }
   ],
   "source": [
    "# to train the pairwise model, we define which feature categories to use\n",
    "# here it is all of them\n",
    "features_to_use = [\n",
    "    \"name_similarity\",\n",
    "    \"affiliation_similarity\",\n",
    "    \"email_similarity\",\n",
    "    \"coauthor_similarity\",\n",
    "    \"venue_similarity\",\n",
    "    \"year_diff\",\n",
    "    \"title_similarity\",\n",
    "    \"reference_features\",\n",
    "    \"misc_features\",\n",
    "    \"name_counts\",\n",
    "    \"embedding_similarity\",\n",
    "    \"journal_similarity\",\n",
    "    \"advanced_name_similarity\",\n",
    "]\n",
    "\n",
    "# we also have this special second \"nameless\" model that doesn't use any name-based features\n",
    "# it helps to improve clustering performance by preventing model overreliance on names\n",
    "nameless_features_to_use = [\n",
    "    feature_name\n",
    "    for feature_name in features_to_use\n",
    "    if feature_name not in {\"name_similarity\", \"advanced_name_similarity\", \"name_counts\"}\n",
    "]\n",
    "\n",
    "# we store all the information about the features in this convenient wrapper\n",
    "featurization_info = FeaturizationInfo(features_to_use=features_to_use, featurizer_version=FEATURIZER_VERSION)\n",
    "nameless_featurization_info = FeaturizationInfo(features_to_use=nameless_features_to_use, featurizer_version=FEATURIZER_VERSION)\n",
    "\n",
    "# now we can actually go and get the pairwise training, val and test data\n",
    "train, val, test = featurize(anddata, featurization_info, n_jobs=4, use_cache=False, chunk_size=DEFAULT_CHUNK_SIZE, nameless_featurizer_info=nameless_featurization_info, nan_value=np.nan)  # type: ignore\n",
    "X_train, y_train, nameless_X_train = train\n",
    "X_val, y_val, nameless_X_val = val\n",
    "X_test, y_test, nameless_X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:51<00:00,  4.46s/trial, best loss: -0.961766757760418]\n",
      "100%|██████████| 25/25 [01:39<00:00,  3.99s/trial, best loss: -0.8150096589558353]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x7fa72533f6d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we define and fit the pairwise modelers\n",
    "pairwise_modeler = PairwiseModeler(\n",
    "    n_iter=25,  # number of hyperparameter search iterations\n",
    "    estimator=None,  # this will use the default LightGBM classifier\n",
    "    search_space=None,  # this will use the default LightGBM search space\n",
    "    monotone_constraints=featurization_info.lightgbm_monotone_constraints,  # we use monotonicity constraints to make the model more sensible\n",
    "    random_state=random_seed,\n",
    ")\n",
    "pairwise_modeler.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "# as mentioned above, there are 2: one with all features and a nameless one\n",
    "nameless_pairwise_modeler = PairwiseModeler(\n",
    "    n_iter=25,\n",
    "    estimator=None,\n",
    "    search_space=None,\n",
    "    monotone_constraints=nameless_featurization_info.lightgbm_monotone_constraints,\n",
    "    random_state=random_seed,\n",
    ")\n",
    "nameless_pairwise_modeler.fit(nameless_X_train, y_train, nameless_X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 12:38:22,052 - s2and - INFO - Fitting clusterer\n",
      "2020-12-16 12:38:22,093 - s2and - INFO - Making 10 distance matrices\n",
      "2020-12-16 12:38:22,094 - s2and - INFO - Initializing pairwise_probas\n",
      "2020-12-16 12:38:22,096 - s2and - INFO - Pairwise probas initialized, starting making all pairs\n",
      "2020-12-16 12:38:22,096 - s2and - INFO - Featurizing batch 0\n",
      "2020-12-16 12:38:22,098 - s2and - INFO - Getting constraints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 1728 iterations\n",
      "Finished loading model, total used 1242 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 12:38:22,313 - s2and - INFO - Creating 160277 pieces of work\n",
      "Creating work: 160277it [00:00, 724510.10it/s]\n",
      "2020-12-16 12:38:22,537 - s2and - INFO - Created pieces of work\n",
      "2020-12-16 12:38:22,538 - s2and - INFO - Cached changed, doing 160277 work in parallel\n",
      "Doing work: 100%|██████████| 160277/160277 [00:06<00:00, 24641.02it/s]\n",
      "2020-12-16 12:38:32,758 - s2and - INFO - Work completed\n",
      "2020-12-16 12:38:32,759 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2020-12-16 12:38:32,872 - s2and - INFO - Numpy arrays made\n",
      "2020-12-16 12:38:32,884 - s2and - INFO - Making predict flags\n",
      "2020-12-16 12:38:32,910 - s2and - INFO - Pairwise classification\n",
      "2020-12-16 12:38:36,240 - s2and - INFO - Starting to make matrices\n",
      "Writing matrices: 100%|██████████| 160277/160277 [00:00<00:00, 1036705.22it/s]\n",
      "2020-12-16 12:38:36,400 - s2and - INFO - 10 distance matrices made\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 57.90trial/s, best loss: -0.887]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 12:38:36,882 - s2and - INFO - Clusterer fit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<s2and.model.Clusterer at 0x7fa6a5b8eb00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can fit the clusterer itself\n",
    "clusterer = Clusterer(\n",
    "    featurization_info,\n",
    "    pairwise_modeler.classifier,  # the actual pairwise classifier\n",
    "    cluster_model=FastCluster(linkage='average'),  # average linkage agglomerative clustering\n",
    "    search_space={\"eps\": hp.uniform(\"choice\", 0, 1)},  # the hyperparemetrs for the clustering algorithm\n",
    "    n_jobs=n_jobs,\n",
    "    use_cache=False,\n",
    "    nameless_classifier=nameless_pairwise_modeler.classifier,  # the nameless pairwise classifier\n",
    "    nameless_featurizer_info=nameless_featurization_info,\n",
    "    random_state=random_seed,\n",
    "    use_default_constraints_as_supervision=False,  # this is an option used by the S2 production system but not in the S2AND paper\n",
    ")\n",
    "clusterer.fit(anddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.995, 'Average Precision': 0.996, 'F1': 0.947, 'Precision': 0.944, 'Recall': 0.954}\n"
     ]
    }
   ],
   "source": [
    "# but how good are our models? \n",
    "# first, let's look at the quality of the pairwise evaluation\n",
    "pairwise_metrics = pairwise_eval(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    pairwise_modeler,\n",
    "    os.path.join(PROJECT_ROOT_PATH, \"data\", \"tutorial_figures\"),  # where to put the figures\n",
    "    \"tutorial_figures\",  # what to call the figures\n",
    "    featurization_info.get_feature_names(),\n",
    "    nameless_classifier=nameless_pairwise_modeler,\n",
    "    nameless_X=nameless_X_test,\n",
    "    nameless_feature_names=nameless_featurization_info.get_feature_names(),\n",
    "    skip_shap=False,  # if your model isn't a tree-based model, you should put True here and it will not make SHAP figures\n",
    ")\n",
    "print(pairwise_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 12:39:09,201 - s2and - INFO - Making 13 distance matrices\n",
      "2020-12-16 12:39:09,202 - s2and - INFO - Initializing pairwise_probas\n",
      "2020-12-16 12:39:09,203 - s2and - INFO - Pairwise probas initialized, starting making all pairs\n",
      "2020-12-16 12:39:09,204 - s2and - INFO - Featurizing batch 0\n",
      "2020-12-16 12:39:09,204 - s2and - INFO - Getting constraints\n",
      "2020-12-16 12:39:09,263 - s2and - INFO - Creating 44855 pieces of work\n",
      "2020-12-16 12:39:09,296 - s2and - INFO - Created pieces of work\n",
      "2020-12-16 12:39:09,296 - s2and - INFO - Cached changed, doing 44855 work in parallel\n",
      "Doing work: 100%|██████████| 44855/44855 [00:01<00:00, 27100.38it/s]\n",
      "2020-12-16 12:39:14,681 - s2and - INFO - Work completed\n",
      "2020-12-16 12:39:14,682 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2020-12-16 12:39:14,723 - s2and - INFO - Numpy arrays made\n",
      "2020-12-16 12:39:14,729 - s2and - INFO - Making predict flags\n",
      "2020-12-16 12:39:14,737 - s2and - INFO - Pairwise classification\n",
      "2020-12-16 12:39:15,919 - s2and - INFO - Starting to make matrices\n",
      "Writing matrices: 100%|██████████| 44855/44855 [00:00<00:00, 1000444.06it/s]\n",
      "2020-12-16 12:39:15,967 - s2and - INFO - 13 distance matrices made\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B3 (P, R, F1)': (0.92, 0.985, 0.951), 'Cluster (P, R F1)': (0.979, 0.997, 0.988), 'Cluster Macro (P, R, F1)': (0.911, 0.977, 0.938), 'Pred bigger ratio (mean, count)': (1.86, 609), 'True bigger ratio (mean, count)': (1.76, 36)}\n"
     ]
    }
   ],
   "source": [
    "# we can do the same thing for the clustering performance\n",
    "cluster_metrics, b3_metrics_per_signature = cluster_eval(\n",
    "    anddata,\n",
    "    clusterer,\n",
    "    split=\"test\",  # which part of the data to evaluate on, can also be 'val'\n",
    "    use_s2_clusters=False,  # set to true if you want to see how the old S2 system does\n",
    ")\n",
    "print(cluster_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to reproduce the facet plots from the S2AND paper, check out the `facet_eval` function also!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2and",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0 (default, Oct  9 2018, 10:31:47) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "619898e886546317b75a26a9a71a60d0160543b2681dcfacaa678505c50cf13f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
